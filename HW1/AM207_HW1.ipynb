{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1 (Due 09/18/2019, 11:59pm)\n",
    "## Maximum Likelihood Learning and Bayesian Inference\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Dimitris Vamvourellis**\n",
    "\n",
    "**Students collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. Portions of your submission that contains syntactic or run-time errors will not be graded.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import math as ma\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "In the competitive rubber chicken retail market, the success of a company is built on satisfying the exacting standards of a consumer base with refined and discriminating taste. In particular, customer product reviews are all important. But how should we judge the quality of a product based on customer reviews?\n",
    "\n",
    "On Amazon, the first customer review statistic displayed for a product is the ***average rating***. The following are the main product pages for two competing rubber chicken products, manufactured by Lotus World and Toysmith respectively:\n",
    "\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus1.png) |  ![alt](toysmith1.png)\n",
    "\n",
    "Clicking on the 'customer review' link on the product pages takes us to a detailed break-down of the reviews. In particular, we can now see the number of times a product is rated a given rating (between 1 and 5 stars).\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus2.png) |  ![alt](toysmith2.png)\n",
    "\n",
    "\n",
    "In the following, we will ask you to build statistical models to compare these two products using the observed rating. Larger versions of the images are available in the data set accompanying this notebook.\n",
    "\n",
    "\n",
    "\n",
    "## Part I: A Maximum Likelihood Model\n",
    "1. **(Model Building)** Suppose that for each product, we can model the probability of the value each new rating as the following vector:\n",
    "$$\n",
    "\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5]\n",
    "$$\n",
    "  where $\\theta_i$ is the probability that a given customer will give the product $i$ number of stars. That is, each new rating (a value between 1 and 5) has a categorical distribution $Cat(\\theta)$. Represent the observed ratings of an Amazon product as a vector $R = [r_1, r_2, r_3, r_4, r_5]$ where, for example, $r_4$ is the number of $4$-star reviews out of a total of $N$ ratings. Write down the likelihood of $R$. That is, what is $p(R| \\theta)$?\n",
    "\n",
    "  **Note:** The observed ratings for each product should be read off the image files included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** \n",
    "\n",
    "The likelihood of R given N observations can be written using the pmf of the Multinomial distribution as follows:\n",
    "\n",
    "$$p(R| \\theta) = \\frac{N!}{r_1!r_2!r_3!r_4!r_5!}\\theta_1^{r_1}\\theta_2^{r_2}\\theta_3^{r_3}\\theta_4^{r_4}\\theta_5^{r_5}$$\n",
    "\n",
    "Hence, the log-likelihood is given by\n",
    "$$\\ell(\\theta) = \\log{p(R| \\theta)} = \\log(N!)-\\displaystyle\\sum_{i=1}^{5}{\\log(r_i!)}+ \\displaystyle\\sum_{i=1}^{5}{r_i \\log(\\theta_i)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Model Fitting)** Find the maximum likelihood estimator of $\\theta$ for the Lotus World model; find the MLE of $\\theta$ for the Toysmith model. You need to make a reasonably mathematical argument for why your estimate actually maximizes the likelihood (i.e. recall the criteria for a point to be a global optima of a function).\n",
    "\n",
    "  *Note:* I recommend deriving the MLE using the general expression of the likelihood. That is, derive the posterior using the variable $R$, then afterwards plug in your specific values of $R$ for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "To derive the MLE of the $\\theta$ vector which maximizes the likelihood $p(R| \\theta)$, we need to solve the following constrained optimization problem:\n",
    "$$\n",
    "\\mathrm{max}\\;\\ell(\\theta),\\quad\\displaystyle\\sum_{i=1}^{5}{\\theta_i}=1\n",
    "$$\n",
    "whose Lagrangian is given by:\n",
    "$$\n",
    "J(\\theta, \\lambda) = \\ell(\\theta) - \\lambda(\\displaystyle\\sum_{i=1}^{5}{\\theta_i} - 1).\n",
    "$$\n",
    "\n",
    "The gradient of the Lagrangian $J$ with respect to $(\\theta, \\lambda)$ is the vector $\\nabla_{(\\theta, \\lambda)} J(\\theta,\\lambda) = \\left[\\frac{\\partial J}{\\partial \\theta_1}, \\frac{\\partial J}{\\partial \\theta_2}, \\frac{\\partial J}{\\partial \\theta_3}, \\frac{\\partial J}{\\partial \\theta_4}, \\frac{\\partial J}{\\partial \\theta_5}, \\frac{\\partial J}{\\partial \\lambda} \\right]$, where the partial derivatives are given by:\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial \\theta_1} &= \\frac{r_1}{\\theta_1} - \\lambda\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_2} &= \\frac{r_2}{\\theta_2} - \\lambda\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_3} &= \\frac{r_3}{\\theta_3} - \\lambda\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} &= \\frac{r_4}{\\theta_4} - \\lambda\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} &= \\frac{r_5}{\\theta_5} - \\lambda\\\\\n",
    "\\frac{\\partial J}{\\partial \\lambda} &= \\theta_1 + \\theta_2 + \\theta_3 + \\theta_4 + \\theta_5 - 1\n",
    "\\end{aligned}\n",
    "\n",
    " \n",
    "The stationary points of the Lagrangian can be obtained by solving the following system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial J}{\\partial \\theta_1} &= \\frac{r_1}{\\theta_1} - \\lambda= 0\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_2} &= \\frac{r_2}{\\theta_2} - \\lambda= 0\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_3} &= \\frac{r_3}{\\theta_3} - \\lambda= 0\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} &= \\frac{r_4}{\\theta_4} - \\lambda= 0\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} &= \\frac{r_5}{\\theta_5} - \\lambda= 0\\\\\n",
    "\\frac{\\partial J}{\\partial \\lambda} &= \\theta_1 + \\theta_2 + \\theta_3 + \\theta_4 + \\theta_5 - 1= 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Solving this system, we get a unique solution at:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\theta_1 = \\frac{r_1}{\\lambda}&\\\\\n",
    "\\theta_2 = \\frac{r_2}{\\lambda}&\\\\\n",
    "\\theta_3 = \\frac{r_3}{\\lambda}&\\\\\n",
    "\\theta_4 = \\frac{r_4}{\\lambda}&\\\\\n",
    "\\theta_5 = \\frac{r_5}{\\lambda}&\\\\\n",
    "\\theta_1 + \\theta_2 + \\theta_3 + \\theta_4 +\\theta_5  = 1&\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where, $\\lambda=r_1+r_2+r_3+r_4+r_5=N$.\n",
    "\n",
    "**Characterize Stationary Point**\n",
    "\n",
    "We have a unique stationary point but we need to check whether this is a maximum or minimum of the log likelihood. Testing the value of the log-likelihood at the stationary point of the Lagrangian (where $\\theta_i = \\frac{r_i}{N}$), and another point on the line $\\theta_1 + \\theta_2 + \\theta_3 + \\theta_4 + \\theta_5  = 1$, we can check whether this is a minimum or maximum. If the likelihood is greater at the stationary point, then this is a maximum. Then, since there is only one optimum, it has to be a global optimum. For example, we can do this check by calculating the likelihood using LW actual ratings as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above result, we know that the MLE of $\\theta$ is given by the vector which holds the proportions of each rating. Therefore, for the Lotus World model, the MLE of $\\theta$ is given by\n",
    "$$\\theta_{MLE_{LW}} = [0.06, 0.04, 0.06, 0.17, 0.67].$$\n",
    "\n",
    "Respectively, for the Toysmith model, the MLE of $\\theta$ is given by\n",
    "$$\\theta_{MLE_{Toysmith}} = [0.14, 0.08, 0.07, 0.11, 0.60].$$\n",
    "\n",
    "We will plug $\\theta_{MLE_{LW}}$ in the log-likelihood and compare its value with the log-likelihood when plugging another arbitrary point $\\theta^{\\prime} = [0.2, 0.2, 0.2, 0.2, 0.2].$ The only term of the log likelihood that depends on $\\theta$ is $\\displaystyle\\sum_{i=1}^{5}{r_i \\log(\\theta_i)}$. Hence we can do the comparisons based on this term.\n",
    "\n",
    "For this, we will need the approximate absolute counts of ratings per number of stars given, which we will calculate below for LW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus World: 5 stars=108.000000, 4 stars=28.000000, 3 stars=10.000000, 2 stars=6.000000  1 star=10.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Lotus World: 5 stars=%f, 4 stars=%f, 3 stars=%f, 2 stars=%f  1 star=%f\" \n",
    "     % (round(0.667*162),round(0.17*162),round(0.06*162),round(0.04*162),round(0.06*162)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-168.44783805099195 -260.72894181432423\n"
     ]
    }
   ],
   "source": [
    "R_test = np.array([10, 6, 10, 28, 108])\n",
    "theta_MLE = np.array([0.06, 0.04, 0.06, 0.17, 0.67])\n",
    "theta_prime = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "sum_mle = np.dot(np.log(theta_MLE), R_test)\n",
    "sum_prime = np.dot(np.log(theta_prime), R_test)\n",
    "print(sum_mle, sum_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, log-likelihood is greater at $\\theta_{MLE_{LW}}$, hence the log-likelihood is maximized and this is a global maximum along the line $\\theta_1 + \\theta_2 + \\theta_3 + \\theta_4 + \\theta_5  = 1$ at the stationary point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Model Interpretation)** Based on your MLE of $\\theta$'s for both models, do you feel confident deciding if one product is superior to another? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't feel confident deciding if one product is superior to the other since there is no striking difference in proportions of ratings for each category of stars. Lotus World product has a slightly larger proportion of 5s and a slightly lower proportion of 1s but this small difference is not enough to claim that one product is superior to the other. The number of ratings that we have for Lotus World is not large enough and significantly lower than the number of ratings for Toysmith. As it is known, the MLE tends to overfit when there is scarcity of data, hence we are not confident that this estimate is close to the true parameter $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: A Bayesian Model\n",
    "\n",
    "1. **(Model Building)** Suppose you are told that customer opinions are very polarized in the retail world of rubber chickens, that is, most reviews will be 5 stars or 1 stars (with little middle ground). What would be an appropriate $\\alpha$ for the Dirichlet prior on $\\theta$? Recall that the Dirichlet pdf is given by:\n",
    "$$\n",
    "p_{\\Theta}(\\theta) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}, \\quad B(\\alpha) = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)},\n",
    "$$\n",
    "where $\\theta_i \\in (0, 1)$ and $\\sum_{i=1}^k \\theta_i = 1$, $\\alpha_i > 0 $ for $i = 1, \\ldots, k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "If our prior belief is that most reviews are either 5 or 1 stars, then we can set a Dirichlet prior as follows:\n",
    "\n",
    "$$\\alpha = [5, 1, 1, 1, 5].$$\n",
    "\n",
    "In this way, we specify our belief that it is equally probable to have 1 star review or a 5 star review, but less mass is allocated in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Inference)** Analytically derive the posterior distribution (using the likelihoods you derived in Part I) for each product.\n",
    "\n",
    "  *Note:* I recommend deriving the posterior using the general expression of a Dirichelet pdf. That is, derive the posterior using the variable $\\alpha$, then afterwards plug in your specific values of $\\alpha$ when you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The posterior distribution is given by\n",
    "\n",
    "$$p(\\theta | R) = \\frac{p(R | \\theta)p(\\theta)}{p(R)} = \\frac{N!\\frac{1}{B(\\alpha)}\\prod_{i=1}^5\\frac{\\theta_i^{r_i}}{r_i!}\\prod_{i=1}^5 \\theta_i^{\\alpha_i - 1}}{p(R)}$$\n",
    "\n",
    "We can bundle together all the constants and rewrite the posterior as\n",
    "\n",
    "$$p(\\theta | R) = const * \\prod_{i=1}^5 \\theta_i^{\\alpha_i+r_i - 1}$$\n",
    "\n",
    "where $const = \\frac{N!\\prod_{i=1}^5\\frac{1}{r_i!}}{B(\\alpha)p(R)}$. We recognize that the posterior is a Dirichlet distribution with parameters given by the vector $R+\\alpha$ and the constant term is the normalization constant. Hence, the posterior is given by\n",
    "\n",
    "$$p(\\theta | R) = \\frac{1}{B(\\alpha + R)} \\prod_{i=1}^5 \\theta_i^{\\alpha_i+r_i - 1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the posterior distributions for each model, we will need the approximate absolute counts of ratings per number of stars given, which we will calculate below for each toy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus World: 5 stars=108.000000, 4 stars=28.000000, 3 stars=10.000000, 2 stars=6.000000  1 star=10.000000\n",
      "Toysmith: 5 stars=246.000000, 4 stars=45.000000, 3 stars=29.000000, 2 stars=33.000000  1 star=57.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Lotus World: 5 stars=%f, 4 stars=%f, 3 stars=%f, 2 stars=%f  1 star=%f\" \n",
    "     % (round(0.667*162),round(0.17*162),round(0.06*162),round(0.04*162),round(0.06*162)))\n",
    "print(\"Toysmith: 5 stars=%f, 4 stars=%f, 3 stars=%f, 2 stars=%f  1 star=%f\" \n",
    "     % (round(0.6*410),round(0.11*410),round(0.07*410),round(0.08*410),round(0.14*410)))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, for a prior Dirichlet disribution with $\\alpha = [5, 1, 1, 1, 5]$ and using the absolute counts we calculated above, the posterior distribution for the Lotus World model is given by\n",
    "\n",
    "$$p(\\theta_{LW} | R_{LW}) = \\frac{1}{B(\\alpha+R_{LW})}\\theta_1^{10+5-1}\\theta_2^{6+1-1}\\theta_3^{10+1-1}\\theta_4^{28+1-1}\\theta_5^{108+5-1}=\\frac{1}{B(\\alpha+R_{LW})}\\theta_1^{14}\\theta_2^{6}\\theta_3^{10}\\theta_4^{28}\\theta_5^{112}$$, where $R_{LW} = [10, 6, 10, 28, 108]$.\n",
    "\n",
    "Respectively, for the same prior, the posterior distribution for the Toysmith model is given by\n",
    "\n",
    "$$p(\\theta_{Toysmith} | R_{Toysmith}) = \\frac{1}{B(\\alpha+R_{Toysmith})}\\theta_1^{61}\\theta_2^{33}\\theta_3^{29}\\theta_4^{45}\\theta_5^{250}$$, where $R_{Toysmith} = [246, 45, 29, 33, 57]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(The Maximum A Posterior Estimate)** Analytically or empirically compute the MAP estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MLE? Just for this problem, compute the MAP estimate of $\\theta$ for each product using a Dirichelet prior with hyperparameters $\\alpha = [1, 1, 1, 1, 1]$. Make a conjecture about the effect of the prior on the difference between the MAP estimates and the MLE's of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Since the posterior is a Dirichlet distribution, we have a closed formula for its mode (i.e. the MAP estimate of $\\theta$). Particularly, the mode for each $\\theta_i$ in $\\theta$ drawn from a Dirichlet($\\alpha+R$) is given by\n",
    "\n",
    "$$\\theta_{i_{MAP}} = \\frac{\\alpha_i+r_i-1}{\\displaystyle\\sum_{i=1}^{5}{(\\alpha_i+r_i-1)}}$$\n",
    "\n",
    "Substituring the values of $\\alpha$ and $R$ for each product, we get the corresponding MAP estimate of $\\theta$ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LW, theta_MAP: [0.08235294 0.03529412 0.05882353 0.16470588 0.65882353]\n",
      "For Toysmith, theta_MAP: [0.14593301 0.07894737 0.06937799 0.1076555  0.59808612]\n"
     ]
    }
   ],
   "source": [
    "#calculating MAP estimated for uneven prior\n",
    "R_LW = np.array([10, 6, 10, 28, 108])\n",
    "R_toy = np.array([57, 33, 29, 45, 246])\n",
    "a_prior = np.array([5, 1, 1, 1, 5])\n",
    "theta_MAP_LW = (R_LW + a_prior - 1)/(R_LW + a_prior - 1).sum()\n",
    "theta_MAP_toy = (R_toy + a_prior - 1)/(R_toy + a_prior - 1).sum()\n",
    "print(\"For LW, theta_MAP:\", theta_MAP_LW)\n",
    "print(\"For Toysmith, theta_MAP:\", theta_MAP_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the MAP estimate for Lotus World and Toysmith respectively is \n",
    "\n",
    "$$\\theta_{MAP_{LW}} = [0.082, 0.035, 0.059, 0.165, 0.659],$$\n",
    "$$\\theta_{MAP_{Toysmith}} = [0.146, 0.079, 0.069, 0.108, 0.598],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whereas the MLE estimates found above are\n",
    "\n",
    "$$\\theta_{MLE_{LW}} = [0.06, 0.04, 0.06, 0.17, 0.67].$$\n",
    "$$\\theta_{MLE_{Toysmith}} = [0.14, 0.08, 0.07, 0.11, 0.60].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is made apparent, there is no big difference between the MLE and the MAP estimates for $\\theta$ in each case. The biggest difference can be observed in $\\theta_1$ which is slightly larger under the MAP estimate and in $\\theta_5$ which is larger under the MLE for the LW product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LW, theta_MAP: [0.0617284  0.03703704 0.0617284  0.17283951 0.66666667]\n",
      "For Toysmith, theta_MAP: [0.13902439 0.0804878  0.07073171 0.1097561  0.6       ]\n"
     ]
    }
   ],
   "source": [
    "#calculating MAP estimates for flat prior\n",
    "R_LW = np.array([10, 6, 10, 28, 108])\n",
    "R_toy = np.array([57, 33, 29, 45, 246])\n",
    "a_prior_flat = np.array([1, 1, 1, 1, 1])\n",
    "theta_MAP_LW = (R_LW + a_prior_flat - 1)/(R_LW + a_prior_flat - 1).sum()\n",
    "theta_MAP_toy = (R_toy + a_prior_flat - 1)/(R_toy + a_prior_flat - 1).sum()\n",
    "print(\"For LW, theta_MAP:\", theta_MAP_LW)\n",
    "print(\"For Toysmith, theta_MAP:\", theta_MAP_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under a flat prior, the MAP estimates are\n",
    "\n",
    "$$\\theta_{MAP_{LW}} = [0.062, 0.037, 0.062, 0.173, 0.667],$$\n",
    "$$\\theta_{MAP_{Toysmith}} = [0.139, 0.080, 0.071, 0.110, 0.6].$$\n",
    "\n",
    "In this case, the MAP estimates are almost equal to the MLE estimates. By using a flat prior, we demonstrate our belief that we don't have any prior knowledge about the distribution of the proportion of ratings (i.e. the symmetric Dirichlet distribution is equivalent to a uniform distribution in higher dimensions). In other words, this is an uninformative prior and for this reason the posterior is dominated by the likelihood after seeing many examples. As a result, the MAP and the MLE estimates almost coincide if we set a completely flat prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **(The Posterior Mean Estimate)** Analytically or empirically compute the posterior mean estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MAP estimates and the MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Again, since the posterior is a Dirichlet$(R+\\alpha)$ distribution, we have a formula for the mean of each $\\theta_i$ in $\\theta$ given by\n",
    "\n",
    "$$\\theta_{i_{mean}} = \\frac{\\alpha_i+r_i}{\\displaystyle\\sum_{i=1}^{5}{(\\alpha_i+r_i)}}$$\n",
    "\n",
    "Substituring the values of $\\alpha$ and $R$ for each product, we get the corresponding posterior mean estimate of $\\theta$ as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LW, theta_mean: [0.08571429 0.04       0.06285714 0.16571429 0.64571429]\n",
      "For Toysmith, theta_mean: [0.1465721  0.08037825 0.07092199 0.10874704 0.59338061]\n"
     ]
    }
   ],
   "source": [
    "a_prior = np.array([5, 1, 1, 1, 5])\n",
    "theta_mean_LW = (R_LW + a_prior)/(R_LW + a_prior).sum()\n",
    "theta_mean_toy = (R_toy + a_prior)/(R_toy + a_prior).sum()\n",
    "print(\"For LW, theta_mean:\", theta_mean_LW)\n",
    "print(\"For Toysmith, theta_mean:\", theta_mean_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate the above result empirically by drawing samples from each posterior and calculating the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LW, posterior mean estimate is [0.08581411 0.0398802  0.06279848 0.16564791 0.6458593 ]\n",
      "For Toysmith, posterior mean estimate is [0.14658839 0.0803993  0.07096977 0.10879263 0.59324991]\n"
     ]
    }
   ],
   "source": [
    "samples_LW = np.random.dirichlet(tuple(a_prior+R_LW), 10000)\n",
    "samples_toy = np.random.dirichlet(tuple(a_prior+R_toy), 10000)\n",
    "print(\"For LW, posterior mean estimate is\",np.mean(samples_LW, axis=0))\n",
    "print(\"For Toysmith, posterior mean estimate is\",np.mean(samples_toy, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **(The Posterior Predictive Estimate)** Sample 1000 rating vectors from the posterior predictive for each product, using the $\\alpha$'s you chose in Problem 1. Use the average of the posterior predictive samples to estimate $\\theta$. How do these estimates compare with the MAP, MLE, posterior mean estimate of $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to draw samples from the posterior predictive distribution\n",
    "def get_post_pred_samples(prior_a, N, R, samples):\n",
    "    post_pred_samples = np.zeros((samples, prior_a.shape[0]))\n",
    "    #draw a sample from the posterior and use it to draw a sample from the multinomial predictive.\n",
    "    for i in range(0,samples):\n",
    "        theta_sample = np.random.dirichlet(tuple(a_prior+R), 1)[0]\n",
    "        post_pred_samples[i] = np.random.multinomial(N, theta_sample)\n",
    "    return post_pred_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred_LW = get_post_pred_samples(a_prior, 162, R_LW, 1000)\n",
    "post_pred_toy = get_post_pred_samples(a_prior, 410, R_toy, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the posterior predictive distribution for LW is [ 13.697   6.671  10.183  27.089 104.36 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean of the posterior predictive distribution for LW is\", np.mean(post_pred_LW, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the posterior predictive distribution for Toysmith is [ 59.775  33.404  29.144  44.093 243.584]\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean of the posterior predictive distribution for Toysmith is\", np.mean(post_pred_toy, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide the mean vectors with the sum of ratings to get another estimate for $\\theta$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LW, the estimate of theta based on the mean of the posterior predictive is [0.08454938 0.04117901 0.06285802 0.16721605 0.64419753]\n",
      "For Toysmith, the estimate of theta based on the mean of the posterior predictive is [0.14579268 0.08147317 0.07108293 0.1075439  0.59410732]\n"
     ]
    }
   ],
   "source": [
    "theta_pred_LW = np.mean(post_pred_LW, axis=0)/162.0\n",
    "theta_pred_toy = np.mean(post_pred_toy, axis=0)/410.0\n",
    "\n",
    "print(\"For LW, the estimate of theta based on the mean of the posterior predictive is\", theta_pred_LW)\n",
    "print(\"For Toysmith, the estimate of theta based on the mean of the posterior predictive is\", theta_pred_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the estimate $\\theta$ for each product respectively given the mean of the posterior predictive distribution is\n",
    "\n",
    "$$\\theta_{pred_{LW}} = [0.085, 0.041, 0.063, 0.167, 0.644],$$\n",
    "$$\\theta_{pred_{Toysmith}} = [0.146, 0.080, 0.071, 0.108, 0.594].$$\n",
    "\n",
    "For comparison purposes, we attach the results from the other methods below.\n",
    "\n",
    "$$\\theta_{mean_{LW}} = [0.086, 0.04, 0.063, 0.166, 0.646],$$\n",
    "$$\\theta_{mean_{Toysmith}} = [0.146, 0.080, 0.070, 0.109, 0.593],$$\n",
    "\n",
    "$$\\theta_{MAP_{LW}} = [0.082, 0.035, 0.059, 0.165, 0.659],$$\n",
    "$$\\theta_{MAP_{Toysmith}} = [0.146, 0.079, 0.069, 0.108, 0.598],$$\n",
    "\n",
    "$$\\theta_{MLE_{LW}} = [0.06, 0.04, 0.06, 0.17, 0.67],$$\n",
    "$$\\theta_{MLE_{Toysmith}} = [0.14, 0.08, 0.07, 0.11, 0.60].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these estimates are very close to the MAP and posterior mean estimates while there is a noticeable difference with the MLE regarding $\\theta_1$ and $\\theta_5$ especially. Also, although the difference with MAP is small, $\\theta_{pred}$ estimates are a bit closer to the mean posterior ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **(Model Evaluation)** Compute the 95% credible interval of $\\theta$ for each product (*Hint: compute the 95% credible interval for each $\\theta_i$, $i=1, \\ldots, 5$*). For which product is the posterior mean and MAP estimate more reliable and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_LW = np.random.dirichlet(tuple(a_prior+R_LW), 10000)\n",
    "samples_toy = np.random.dirichlet(tuple(a_prior+R_toy), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LW, the 2.5th percentile is: [0.04861892 0.01617297 0.03166043 0.11465613 0.57428849]\n",
      "For LW, the 97.5th percentile is: [0.13136833 0.07303282 0.10305547 0.22394475 0.71511221]\n"
     ]
    }
   ],
   "source": [
    "print(\"For LW, the 2.5th percentile is:\", np.percentile(samples_LW, 2.5, axis=0))\n",
    "print(\"For LW, the 97.5th percentile is:\", np.percentile(samples_LW, 97.5, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the 95% confidence interval for each entry in $\\theta_{LW}$ is given by\n",
    "\n",
    "$$[(0.049,0.131),(0.016,0.073),(0.031,0.103),(0.115,0.224),(0.574,0.715)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Toysmith, the 2.5th percentile is: [0.11468113 0.05611783 0.04874188 0.08095351 0.54639157]\n",
      "For Toysmith, the 97.5th percentile is: [0.18121776 0.10714582 0.09712603 0.14043751 0.64037328]\n"
     ]
    }
   ],
   "source": [
    "print(\"For Toysmith, the 2.5th percentile is:\", np.percentile(samples_toy, 2.5, axis=0))\n",
    "print(\"For Toysmith, the 97.5th percentile is:\", np.percentile(samples_toy, 97.5, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the 95% confidence interval for each entry in $\\theta_{Toysmith}$ is given by\n",
    "\n",
    "$$[(0.115,0.181),(0.056,0.107),(0.049,0.097),(0.081,0.140),(0.546,0.640)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the 95% confidence intervals provided above, for most of the parameters and especially for $\\theta_1$ and $\\theta_5$, the confidence intervals for Toysmith are tighter. This is expected since we had significantly more observations for Toysmith, hence the posterior distribution is more peaked at the MAP. For this reason, the posterior mean or the MAP estimate is more reliable for Toysmith, since the uncertainty around them is smaller compared to the Lotus World product (as it is reflected in the corresponding 95% intervals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Comparison\n",
    "1. **(Summarizing Customer Ratings)** Recall that on Amazon, the first customer review statistic displayed for a product is the average rating. Name at least one problem with ranking products based on the average customer rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The average rating can be very misleading and inconsistent when the total number of ratings is small. In an extreme scenario, if we had only one rating of 5 stars, the average would be 5 stars. If one more rating of 1 stars was added, the average would change to 3 which gives a totally different sentiment to the buyer. In other words, the average rating does not depict the total number of reviews taken into account and in turn when seeing it in isolation it does not give any sense of the uncertainty around it. For example, let product A have an average rating of 5 stars from 1 review and product B have an average rating of 4.5 stars from 100 reviews. Checking the average rating in isolation would lead us to believe that product A is better. However, in reality, the uncertainty about the true average rating of product A is much greater than the uncertainty for product B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Comparison of Point Estimates)** Which point estimate (MAP, MLE, posterior mean or posterior predictive estimate) of $\\theta$, if any, would you feel choose to rank the two Amazon products? Why? \n",
    "\n",
    "  *Hint: think about which of these estimates are equivalent (if any). If they are not equivalent, what are the special properties of each estimate? What aspect of the data or the model is each estimate good at capturing?*\n",
    "  \n",
    "   **Note:** we're not looking for \"the correct answer\" here. We are looking for a sound decision based on a statistically correct interpretation of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "As mentioned above, I would not choose the MLE in this case since we do not have a significant amount of data and the MLE tends to overfit under scarcity of data. The Bayesian modeling process seems more appealing in this context since we want to have a sense of the uncertainty around the proportion of ratings, given the low number of observations. Also, by choosing the Bayesian modeling process, we are able to encode our realistic prior belief that most of the ratings are polarized (most users would give a rating either if a product is really good or really bad) and thus get a more realistic view of the posterior distribution of $\\theta$. \n",
    "\n",
    "Having this in mind, if I had to choose a point estimate, this would be MAP or the posterior mean since it is based on a distribution which is more realistic given our assumptions (i.e. we have a reasonable prior belief which is updated after observing the data). \n",
    "\n",
    "Generally, it is easier to pick the optimal point estimate if you have a well defined loss function (for example, posterior mean is the optimal point estimate in case of squared loss function). However, in this setting we have not defined such a loss. MAP is the most likely parameter setting given the data, while the posterior mean is the posterior expected value of the parameter. Since the amount of data that we have is not large enough (especially for LW product), the posterior distribution would not be very peaked at the MAP. For this reasons, in this case I would choose the posterior mean estimate since it would be an average of all possible parameter settings weighted by how probable they are. Since, we don't have enough data to be very certain about the point estimate, in this way we would be able to propagate some of the uncertainty in our point estimate by averaging out instead of choosing the most probable from a wide distribution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
